{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.core.algorithms as algos\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import os\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/gender_age_train.csv')\n",
    "train['fold'] = np.arange(train.shape[0])\n",
    "train.fold = (train.fold % 10)+1\n",
    "\n",
    "train[['device_id','fold']].to_csv(\"folds_10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LONAA32\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, must be zero! 0\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "datadir = 'data'\n",
    "gatrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'), index_col='device_id')\n",
    "gatest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'), index_col = 'device_id')\n",
    "phone = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'))\n",
    "# Get rid of duplicate device ids in phone\n",
    "phone = phone.drop_duplicates('device_id',keep='first').set_index('device_id')\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'),  parse_dates=['timestamp'], index_col='event_id')\n",
    "appevents = pd.read_csv(os.path.join(datadir,'app_events.csv'), usecols=['event_id','app_id','is_active'], dtype={'is_active':bool})\n",
    "applabels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "\n",
    "\n",
    "##Load the CV split.\n",
    "folds=pd.read_csv(os.path.join(datadir,\"folds_10.csv\"), index_col='device_id')\n",
    "\n",
    "##Reorder train and cv so the device ids match afterwards\n",
    "gatrain=gatrain.sort_index()\n",
    "folds=folds.sort_index()\n",
    "\n",
    "print(\"validation, must be zero!\", sum(gatrain.index!=folds.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Phone brand\n",
    "#As preparation I create two columns that show which train or test set row a particular device_id belongs to.\n",
    "\n",
    "gatrain['trainrow'] = np.arange(gatrain.shape[0])\n",
    "gatest['testrow'] = np.arange(gatest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand features: train shape (74645, 131), test shape (112071, 131)\n",
      "Model features: train shape (74645, 1667), test shape (112071, 1667)\n"
     ]
    }
   ],
   "source": [
    "# A sparse matrix of features can be constructed in various ways. I use this constructor:\n",
    "# csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n",
    "# where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n",
    "# relationship ``a[row_ind[k], col_ind[k]] = data[k]``\n",
    "#\n",
    "# It lets me specify which values to put into which places in a sparse matrix. For phone brand data the data array will be all ones,\n",
    "# row_ind will be the row number of a device and col_ind will be the number of brand.\n",
    "\n",
    "# Device brand\n",
    "brandencoder = LabelEncoder().fit(phone.phone_brand)\n",
    "phone['brand'] = brandencoder.transform(phone['phone_brand'])\n",
    "gatrain['brand'] = phone['brand']\n",
    "gatest['brand'] = phone['brand']\n",
    "Xtr_brand = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                       (gatrain.trainrow, gatrain.brand)))\n",
    "Xte_brand = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                       (gatest.testrow, gatest.brand)))\n",
    "print('Brand features: train shape {}, test shape {}'.format(Xtr_brand.shape, Xte_brand.shape))\n",
    "\n",
    "# Device model\n",
    "m = phone.phone_brand.str.cat(phone.device_model)\n",
    "\n",
    "modelencoder = LabelEncoder().fit(m)\n",
    "phone['model'] = modelencoder.transform(m)\n",
    "gatrain['model'] = phone['model']\n",
    "gatest['model'] = phone['model']\n",
    "\n",
    "Xtr_model = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                       (gatrain.trainrow, gatrain.model)))\n",
    "Xte_model = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                       (gatest.testrow, gatest.model)))\n",
    "print('Model features: train shape {}, test shape {}'.format(Xtr_model.shape, Xte_model.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model frequency features: train shape (74645, 312), test shape (112071, 312)\n"
     ]
    }
   ],
   "source": [
    "# Calculate model frequency\n",
    "model_freq = phone[\"model\"].value_counts().to_frame()\n",
    "mf_encoder = LabelEncoder().fit(model_freq.model)\n",
    "model_freq['model_freq']=mf_encoder.transform(model_freq['model'])\n",
    "model_freq= model_freq.drop(\"model\", 1)\n",
    "\n",
    "gatrain=gatrain.merge(model_freq, how='left', left_on=\"model\", right_index=True)\n",
    "gatest=gatest.merge(model_freq, how='left', left_on=\"model\", right_index=True)\n",
    "gatest[\"model_freq\"]=gatest[\"model_freq\"].fillna(1) # fill not found frequencies with 1\n",
    "\n",
    "\n",
    "Xtr_model_freq = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                       (gatrain.trainrow, gatrain[\"model_freq\"])))\n",
    "Xte_model_freq = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                       (gatest.testrow, gatest[\"model_freq\"])))\n",
    "\n",
    "print('Model frequency features: train shape {}, test shape {}'.format(Xtr_model_freq.shape, Xte_model_freq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand frequency features: train shape (74645, 73), test shape (112071, 73)\n"
     ]
    }
   ],
   "source": [
    "brand_freq = phone[\"brand\"].value_counts().to_frame()\n",
    "bf_encoder = LabelEncoder().fit(brand_freq.brand)\n",
    "brand_freq['brand_freq']=bf_encoder.transform(brand_freq['brand'])\n",
    "brand_freq= brand_freq.drop(\"brand\", 1)\n",
    "\n",
    "brand_freq.columns.values[0]='brand_freq'\n",
    "gatrain=gatrain.merge(brand_freq, how='left', left_on=\"brand\", right_index=True)\n",
    "gatest=gatest.merge(brand_freq, how='left', left_on=\"brand\", right_index=True)\n",
    "gatest[\"brand_freq\"]=gatest[\"brand_freq\"].fillna(1) # fill not found frequencies with 1\n",
    "\n",
    "Xtr_brand_freq = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                       (gatrain.trainrow, gatrain.brand_freq)))\n",
    "\n",
    "Xte_brand_freq = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                       (gatest.testrow, gatest.brand_freq)))\n",
    "\n",
    "print('Brand frequency features: train shape {}, test shape {}'.format(Xtr_brand_freq.shape, Xte_brand_freq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>trainrow</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_freq</th>\n",
       "      <th>brand_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9223067244542181226</th>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>M23-26</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>257</td>\n",
       "      <td>120</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9222956879900151005</th>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>347</td>\n",
       "      <td>305</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9222754701995937853</th>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>M29-31</td>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>1513</td>\n",
       "      <td>132</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9222352239947207574</th>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>M23-26</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>857</td>\n",
       "      <td>286</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9222173362545970626</th>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>F43+</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>347</td>\n",
       "      <td>305</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9220914901466458680</th>\n",
       "      <td>M</td>\n",
       "      <td>34</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>74640</td>\n",
       "      <td>128</td>\n",
       "      <td>1654</td>\n",
       "      <td>264</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221152396628736959</th>\n",
       "      <td>M</td>\n",
       "      <td>21</td>\n",
       "      <td>M22-</td>\n",
       "      <td>74641</td>\n",
       "      <td>51</td>\n",
       "      <td>846</td>\n",
       "      <td>309</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221608286127666096</th>\n",
       "      <td>F</td>\n",
       "      <td>43</td>\n",
       "      <td>F43+</td>\n",
       "      <td>74642</td>\n",
       "      <td>7</td>\n",
       "      <td>120</td>\n",
       "      <td>205</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221843411551060582</th>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>M22-</td>\n",
       "      <td>74643</td>\n",
       "      <td>120</td>\n",
       "      <td>1561</td>\n",
       "      <td>43</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222849349208140841</th>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>M23-26</td>\n",
       "      <td>74644</td>\n",
       "      <td>51</td>\n",
       "      <td>848</td>\n",
       "      <td>307</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74645 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     gender  age   group  trainrow  brand  model  model_freq  \\\n",
       "device_id                                                                      \n",
       "-9223067244542181226      M   24  M23-26         0     13    257         120   \n",
       "-9222956879900151005      M   36  M32-38         1     15    347         305   \n",
       "-9222754701995937853      M   29  M29-31         2    117   1513         132   \n",
       "-9222352239947207574      M   23  M23-26         3     51    857         286   \n",
       "-9222173362545970626      F   56    F43+         4     15    347         305   \n",
       "...                     ...  ...     ...       ...    ...    ...         ...   \n",
       " 9220914901466458680      M   34  M32-38     74640    128   1654         264   \n",
       " 9221152396628736959      M   21    M22-     74641     51    846         309   \n",
       " 9221608286127666096      F   43    F43+     74642      7    120         205   \n",
       " 9221843411551060582      M   17    M22-     74643    120   1561          43   \n",
       " 9222849349208140841      M   23  M23-26     74644     51    848         307   \n",
       "\n",
       "                      brand_freq  \n",
       "device_id                         \n",
       "-9223067244542181226          69  \n",
       "-9222956879900151005          71  \n",
       "-9222754701995937853          66  \n",
       "-9222352239947207574          72  \n",
       "-9222173362545970626          71  \n",
       "...                          ...  \n",
       " 9220914901466458680          67  \n",
       " 9221152396628736959          72  \n",
       " 9221608286127666096          68  \n",
       " 9221843411551060582          64  \n",
       " 9222849349208140841          72  \n",
       "\n",
       "[74645 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gatrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apps data: train shape (74645, 19237), test shape (112071, 19237)\n"
     ]
    }
   ],
   "source": [
    "# Installed apps features\n",
    "# For each device I want to mark which apps it has installed. So I'll have as many feature columns as there are distinct apps.\n",
    "# Apps are linked to devices through events. So I do the following:\n",
    "# merge device_id column from events table to app_events\n",
    "# group the resulting dataframe by device_id and app and aggregate\n",
    "# merge in trainrow and testrow columns to know at which row to put each device in the features matrix\n",
    "\n",
    "appencoder = LabelEncoder().fit(appevents.app_id)\n",
    "appevents['app'] = appencoder.transform(appevents.app_id)\n",
    "napps = len(appencoder.classes_)\n",
    "deviceapps = (appevents.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                       .groupby(['device_id','app'])['app'].agg(['max'])\n",
    "                       .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                       .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                       .reset_index())\n",
    "\n",
    "d = deviceapps.dropna(subset=['trainrow'])\n",
    "Xtr_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.app)),\n",
    "                      shape=(gatrain.shape[0],napps))\n",
    "d = deviceapps.dropna(subset=['testrow'])\n",
    "Xte_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)),\n",
    "                      shape=(gatest.shape[0],napps))\n",
    "print('Apps data: train shape {}, test shape {}'.format(Xtr_app.shape, Xte_app.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels data: train shape (74645, 492), test shape (112071, 492)\n"
     ]
    }
   ],
   "source": [
    "# App labels features\n",
    "# These are constructed in a way similar to apps features by merging app_labels with the deviceapps dataframe we created above.\n",
    "applabels = applabels.loc[applabels.app_id.isin(appevents.app_id.unique())]\n",
    "applabels['app'] = appencoder.transform(applabels.app_id)\n",
    "labelencoder = LabelEncoder().fit(applabels.label_id)\n",
    "applabels['label'] = labelencoder.transform(applabels.label_id)\n",
    "nlabels = len(labelencoder.classes_)\n",
    "\n",
    "devicelabels = (deviceapps[['device_id','app']]\n",
    "                .merge(applabels[['app','label']])\n",
    "                .groupby(['device_id','label'])['app'].agg(['size'])\n",
    "                .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                .reset_index())\n",
    "devicelabels.head()\n",
    "\n",
    "d = devicelabels.dropna(subset=['trainrow'])\n",
    "Xtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)),\n",
    "                      shape=(gatrain.shape[0],nlabels))\n",
    "d = devicelabels.dropna(subset=['testrow'])\n",
    "Xte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)),\n",
    "                      shape=(gatest.shape[0],nlabels))\n",
    "print('Labels data: train shape {}, test shape {}'.format(Xtr_label.shape, Xte_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels data: train shape (74645, 1), test shape (112071, 1)\n",
      "Labels data: train shape (74645, 24), test shape (112071, 24)\n"
     ]
    }
   ],
   "source": [
    "events_cout = (events.groupby('device_id')['timestamp'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "events_cout.size = (np.log((events_cout['size'])))\n",
    "events_cout.size = events_cout.size/events_cout.size.max()\n",
    "\n",
    "d = events_cout.dropna(subset=['trainrow'])\n",
    "Xtr_eventsize = csr_matrix((d.iloc[:,1], (d.trainrow, np.zeros(d.shape[0]))),\n",
    "                      shape=(gatrain.shape[0],1))\n",
    "\n",
    "d = events_cout.dropna(subset=['testrow'])\n",
    "Xte_eventsize = csr_matrix((d.iloc[:,1], (d.testrow, np.zeros(d.shape[0]))),\n",
    "                      shape=(gatest.shape[0],1))\n",
    "print('Labels data: train shape {}, test shape {}'.format(Xtr_eventsize.shape, Xte_eventsize.shape))\n",
    "\n",
    "events['hour'] = events.timestamp.apply(lambda x: x.hour)\n",
    "events_cout_hourofday = (events.groupby(['device_id','hour'])['hour'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "d = events_cout_hourofday.dropna(subset=['trainrow'])\n",
    "Xtr_event_on_hourofday = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.hour)),\n",
    "                      shape=(gatrain.shape[0],d.hour.nunique()))\n",
    "\n",
    "d = events_cout_hourofday.dropna(subset=['testrow'])\n",
    "Xte_event_on_hourofday = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.hour)),\n",
    "                      shape=(gatest.shape[0],d.hour.nunique()))\n",
    "print('Labels data: train shape {}, test shape {}'.format(Xtr_event_on_hourofday.shape, Xte_event_on_hourofday.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for tr_with_event, must be zero! 0\n",
      "validation for te_with_event, must be zero! 0\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Spliting features\n",
    "#####\n",
    "\n",
    "### Create flag \"with event\":\n",
    "\n",
    "events_grouped = (events.groupby(['device_id'], as_index=False).agg(\n",
    "    {'timestamp':'count'}))\n",
    "events_grouped.columns = ['device_id','with_event']\n",
    "events_grouped['with_event']=1\n",
    "events_grouped=events_grouped.set_index(\"device_id\")\n",
    "\n",
    "tr_with_event=pd.merge(gatrain[[]], events_grouped, how=\"left\", left_index=True, right_index=True)\n",
    "te_with_event=pd.merge(gatest[[]], events_grouped, how=\"left\", left_index=True, right_index=True)\n",
    "tr_with_event[\"with_event\"]=tr_with_event[\"with_event\"].fillna(0)\n",
    "te_with_event[\"with_event\"]=te_with_event[\"with_event\"].fillna(0)\n",
    "\n",
    "print(\"validation for tr_with_event, must be zero!\", sum(gatrain.index!=tr_with_event.index))\n",
    "print(\"validation for te_with_event, must be zero!\", sum(gatest.index!=te_with_event.index))\n",
    "\n",
    "#Add to features just in case\n",
    "Xtr_with_event = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                           (gatrain.trainrow, tr_with_event[\"with_event\"].astype(int))))\n",
    "Xte_with_event = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                           (gatest.testrow, te_with_event[\"with_event\"].astype(int))))\n",
    "########## End of Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Read App Labels\n",
      "# Read App Events\n",
      "# Read Events\n",
      "# Read Phone Brand\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#   App Labels\n",
    "##################\n",
    "\n",
    "print(\"# Read App Labels\")\n",
    "app_lab = pd.read_csv(\"data/app_labels.csv\")\n",
    "app_lab = app_lab.groupby(\"app_id\")[\"label_id\"].apply(\n",
    "    lambda x: \" \".join(str(s) for s in x))\n",
    "\n",
    "##################\n",
    "#   App Events\n",
    "##################\n",
    "print(\"# Read App Events\")\n",
    "app_ev = pd.read_csv(\"data/app_events.csv\")\n",
    "app_ev[\"app_lab\"] = app_ev[\"app_id\"].map(app_lab)\n",
    "app_ev = app_ev.groupby(\"event_id\")[\"app_lab\"].apply(\n",
    "    lambda x: \" \".join(str(s) for s in x))\n",
    "\n",
    "del app_lab\n",
    "\n",
    "##################\n",
    "#     Events\n",
    "##################\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(\"data/events.csv\")\n",
    "events[\"app_lab\"] = events[\"event_id\"].map(app_ev)\n",
    "events = events.groupby(\"device_id\")[\"app_lab\"].apply(\n",
    "    lambda x: \" \".join(str(s) for s in x))\n",
    "\n",
    "del app_ev\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(\"data/phone_brand_device_model.csv\")\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generate Train and Test\n",
      "Before hash: must be zero:  0\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'), index_col='device_id')\n",
    "\n",
    "train[\"dev_id\"]=train.index\n",
    "train[\"app_lab\"] = train[\"dev_id\"].map(events)\n",
    "\n",
    "train=pd.merge(train, pbd, how='left', left_index=True, right_on=\"device_id\")\n",
    "train.index=train[\"dev_id\"]\n",
    "\n",
    "train=train.sort_index()\n",
    "\n",
    "print(\"Before hash: must be zero: \", sum(train.index != gatrain.index))\n",
    "\n",
    "test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'), index_col='device_id')\n",
    "test[\"dev_id\"]=test.index\n",
    "\n",
    "test[\"app_lab\"] = test[\"dev_id\"].map(events)\n",
    "\n",
    "test=pd.merge(test, pbd, how='left', left_index=True, right_on=\"device_id\")\n",
    "\n",
    "del pbd\n",
    "del events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a9efc636954c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mtestrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0msuperrow\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "####Phone brand\n",
    "#\n",
    "def get_hash_data(train, test):\n",
    "    df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "    split_len = len(train)\n",
    "\n",
    "    # TF-IDF Feature\n",
    "    tfv = TfidfVectorizer(min_df=1)\n",
    "    df = df[[\"phone_brand\", \"device_model\", \"app_lab\"]].astype(np.str).apply(\n",
    "        lambda x: \" \".join(s for s in x), axis=1).fillna(\"Missing\")\n",
    "    df_tfv = tfv.fit_transform(df)\n",
    "\n",
    "    train = df_tfv[:split_len, :]\n",
    "    test = df_tfv[split_len:, :]\n",
    "    return train, test\n",
    "\n",
    "def get_hash_data2(train, test):\n",
    "    df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "    split_len = len(train)\n",
    "\n",
    "    # TF-IDF Feature\n",
    "    tfv = TfidfVectorizer(min_df=1)\n",
    "    df = df[[\"phone_brand\", \"device_model\"]].astype(np.str).apply(\n",
    "        lambda x: \" \".join(s for s in x), axis=1).fillna(\"Missing\")\n",
    "    df_tfv = tfv.fit_transform(df)\n",
    "\n",
    "    train = df_tfv[:split_len, :]\n",
    "    test = df_tfv[split_len:, :]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "trainrow = np.arange(train.shape[0])\n",
    "testrow = np.arange(test.shape[0])\n",
    "superrow= np.arange(train.shape[0]+ test.shape[0])\n",
    "\n",
    "train_device_id = train[\"device_id\"].values\n",
    "test_device_id = test[\"device_id\"].values\n",
    "\n",
    "train_bag, test_bag = get_hash_data(train,test)\n",
    "\n",
    "#bags only brand and model:\n",
    "train_bag2, test_bag2 = get_hash_data2(train,test)\n",
    "\n",
    "\n",
    "del train\n",
    "del test\n",
    "\n",
    "print(\"After hash: must be zero: \", sum(train_device_id != gatrain.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: train shape (74645, 23984), test shape (112071, 23984)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = hstack((Xtr_brand, Xtr_model, Xtr_brand_freq, Xtr_model_freq, Xtr_app, Xtr_label,Xtr_eventsize,Xtr_event_on_hourofday,\n",
    "                 train_bag, Xtr_with_event), format='csr')\n",
    "Xtest =  hstack((Xte_brand, Xte_model, Xte_brand_freq, Xte_model_freq, Xte_app, Xte_label,Xte_eventsize,Xte_event_on_hourofday,\n",
    "                 test_bag, Xte_with_event), format='csr')\n",
    "\n",
    "\n",
    "Xtrain_ne = hstack((Xtr_brand, Xtr_model, Xtr_brand_freq, Xtr_model_freq, train_bag2, Xtr_with_event), format='csr')\n",
    "Xtest_ne =  hstack((Xte_brand, Xte_model, Xte_brand_freq, Xte_model_freq, test_bag2, Xte_with_event), format='csr')\n",
    "\n",
    "print('All features: train shape {}, test shape {}'.format(Xtrain.shape, Xtest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features after dimensionality reduction: train shape (74645, 18150), test shape (112071, 18150)\n"
     ]
    }
   ],
   "source": [
    "# Reduce dimensionality\n",
    "indices = np.nonzero(Xtrain)\n",
    "columns_non_unique = indices[1]\n",
    "unique_columns = sorted(set(columns_non_unique))\n",
    "Xtrain=Xtrain.tocsc()[:,unique_columns]\n",
    "Xtest=Xtest.tocsc()[:,unique_columns]\n",
    "\n",
    "print('All features after dimensionality reduction: train shape {}, test shape {}'.format(Xtrain.shape, Xtest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Start modeling\n",
    "#################\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "targetencoder = LabelEncoder().fit(gatrain.group)\n",
    "y = targetencoder.transform(gatrain.group)\n",
    "nclasses = len(targetencoder.classes_)\n",
    "\n",
    "##Keras stuff\n",
    "dummy_y = tf.keras.utils.to_categorical(y) ## Funcion de Keras!\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0\n",
    "\n",
    "def baseline_model(num_columns):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.4, input_shape=(num_columns,)))\n",
    "    model.add(Dense(75))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.20))\n",
    "\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#End of keras stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LONAA32\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-events: Logistic logloss for fold 1 is 2.3883394655887376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LONAA32\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With-events: Logistic logloss for fold 1 is 1.964509684745956\n",
      "Total: Logistic logloss for fold 1 is 2.2535542144540077\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d68f238cfb04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     fit = model.fit_generator(generator=batch_generator(Xtr, Ytr_dum, 381, True),\n\u001b[0m\u001b[0;32m     63\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1463\u001b[0m     \"\"\"\n\u001b[0;32m   1464\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1465\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1466\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create predictions repository:\n",
    "pred = np.zeros((y.shape[0],nclasses*2))\n",
    "pred_test = np.zeros((gatest.shape[0],nclasses*2))\n",
    "n_folds=len(folds[\"fold\"].unique())\n",
    "\n",
    "\n",
    "for fold_id in range(1, n_folds + 1):\n",
    "    #fold_id=1\n",
    "    train_id = folds[\"fold\"].values != fold_id\n",
    "    valid_id = folds[\"fold\"].values == fold_id\n",
    "\n",
    "    # With no events\n",
    "    train_id_ne = np.logical_and(train_id, tr_with_event[\"with_event\"].values == 0)\n",
    "    valid_id_ne = np.logical_and(valid_id, tr_with_event[\"with_event\"].values == 0)\n",
    "    test_id_ne = te_with_event[\"with_event\"].values == 0\n",
    "\n",
    "    # With events: Training using only common features\n",
    "    train_id_we = np.logical_and(train_id, tr_with_event[\"with_event\"].values == 1)\n",
    "    valid_id_we = np.logical_and(valid_id, tr_with_event[\"with_event\"].values == 1)\n",
    "    test_id_we = te_with_event[\"with_event\"].values == 1\n",
    "\n",
    "    # First, train on all data, but only no-events feature. Validate with no events:\n",
    "    Xtr, Ytr = Xtrain_ne[train_id, :], y[train_id]\n",
    "    Xva, Yva = Xtrain_ne[valid_id_ne, :], y[valid_id_ne]\n",
    "\n",
    "    # Logistic regression >\n",
    "    clf1 = LogisticRegression(C=0.06, multi_class='multinomial', solver='lbfgs')  # 2.38715733092\n",
    "    # Fitting logistic regression 1\n",
    "    clf1.fit(Xtr, Ytr)\n",
    "\n",
    "    # Predicting only in those with no events!\n",
    "    pred[valid_id_ne, 0:12] = clf1.predict_proba(Xva)\n",
    "    pred_test[test_id_ne, 0:12] = pred_test[test_id_ne, 0:12] + clf1.predict_proba(Xtest_ne[test_id_ne, :])\n",
    "\n",
    "    score_val = log_loss(Yva, pred[valid_id_ne, 0:12])\n",
    "    print(\"No-events: Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    # 2.- After, train only rows with events\n",
    "    Xtr, Ytr = Xtrain[train_id_we, :], y[train_id_we]\n",
    "    Xva, Yva = Xtrain[valid_id_we, :], y[valid_id_we]\n",
    "\n",
    "    clf2 = LogisticRegression(C=0.016, multi_class='multinomial', solver='lbfgs')  # 1.99914889909\n",
    "    clf2.fit(Xtr, Ytr)\n",
    "\n",
    "    # Predicting only in those with events!\n",
    "    pred[valid_id_we, 0:12] = clf2.predict_proba(Xva)\n",
    "    pred_test[test_id_we, 0:12] = pred_test[test_id_we, 0:12] + clf2.predict_proba(Xtest[test_id_we, :])\n",
    "\n",
    "    score_val = log_loss(Yva, pred[valid_id_we, 0:12])\n",
    "    print(\"With-events: Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    Xva, Yva = Xtrain[valid_id, :], y[valid_id]\n",
    "    score_val = log_loss(Yva, pred[valid_id, 0:12])\n",
    "    print(\"Total: Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    ## Fitting Keras! ------------------------------------------------------------------>\n",
    "    # First, train on all data, but only no-events feature. Validate with no events:\n",
    "    Xtr, Ytr_dum = Xtrain_ne[train_id, :], dummy_y[train_id]\n",
    "    Xva, Yva_dum = Xtrain_ne[valid_id_ne, :], dummy_y[valid_id_ne]\n",
    "\n",
    "    model = baseline_model(Xtr.shape[1])\n",
    "    fit = model.fit_generator(generator=batch_generator(Xtr, Ytr_dum, 381, True),\n",
    "                              epochs=20,\n",
    "                              steps_per_epoch=Xtr.shape[0], verbose=2,\n",
    "                              validation_data=(Xva.todense(), Yva_dum)\n",
    "                              )\n",
    "\n",
    "    # evaluate the model\n",
    "    pred[valid_id_ne, 12:25] = model.predict_generator(generator=batch_generatorp(Xva, 400, False),\n",
    "                                                       val_samples=Xva.shape[0])\n",
    "    pred_test[test_id_ne, 12:25] = pred_test[test_id_ne, 12:25] + \\\n",
    "                                   model.predict_generator(\n",
    "                                       generator=batch_generatorp(Xtest_ne[test_id_ne, :], 400, False),\n",
    "                                       val_samples=Xtest_ne[test_id_ne, :].shape[0])\n",
    "\n",
    "    # 2.- After, train all data (keras)\n",
    "    Xtr, Ytr_dum = Xtrain[train_id, :], dummy_y[train_id]\n",
    "    Xva, Yva_dum = Xtrain[valid_id_we, :], dummy_y[valid_id_we]\n",
    "\n",
    "    model = baseline_model(Xtr.shape[1])\n",
    "    fit = model.fit_generator(generator=batch_generator(Xtr, Ytr_dum, 381, True),\n",
    "                              epochs=20,\n",
    "                              steps_per_epoch=Xtr.shape[0], verbose=2,\n",
    "                              validation_data=(Xva.todense(), Yva_dum)\n",
    "                              )\n",
    "\n",
    "    # evaluate the model, and predict only with events:\n",
    "    pred[valid_id_we, 12:25] = model.predict_generator(generator=batch_generatorp(Xva, 400, False),\n",
    "                                                       val_samples=Xva.shape[0])\n",
    "    pred_test[test_id_we, 12:25] = pred_test[test_id_we, 12:25] + \\\n",
    "                                   model.predict_generator(generator=batch_generatorp(Xtest[test_id_we, :], 400, False),\n",
    "                                                           val_samples=Xtest[test_id_we, :].shape[0])\n",
    "\n",
    "    # pred_test[test_id_ne,0:12] = pred_test[test_id_ne,0:12] + clf1.predict_proba(Xtest_ne[test_id_ne, :])\n",
    "\n",
    "    Xva, Yva = Xtrain[valid_id, :], y[valid_id]\n",
    "    score_val = log_loss(Yva, pred[valid_id, 12:25])\n",
    "    print(\"Total: Keras logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "print(\"## End of folds work --------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names=np.concatenate((targetencoder.classes_, targetencoder.classes_), axis=0)\n",
    "\n",
    "##Averaging predictions for all folds in the test set\n",
    "pred_test /= float(n_folds)\n",
    "\n",
    "score_val=log_loss(y, pred[:,0:12])\n",
    "print(\"Logistic: logloss for {} folds is {}\". format(n_folds, score_val))\n",
    "\n",
    "sum(pred[6,12:25])\n",
    "sum(pred_test[1,12:25])\n",
    "\n",
    "\n",
    "score_val=log_loss(y, pred[:,12:25])\n",
    "print(\"Keras: logloss for {} folds is {}\". format(n_folds, score_val))\n",
    "\n",
    "\n",
    "sum(pred_test[1,12:25])\n",
    "\n",
    "pred_train_df = pd.DataFrame(pred, index = gatrain.index, columns=col_names)\n",
    "\n",
    "pred_test_df = pd.DataFrame(pred_test, index = gatest.index, columns=col_names)\n",
    "\n",
    "pred_train_df.to_csv('/home/username/projects/talkingData/keras_pred_train_bags5_wEvents_allData_20160824.csv', index=True, index_label='device_id')\n",
    "pred_test_df.to_csv('/home/username/projects/talkingData/keras_pred_test_bags5_wEvents_allData_20160824.csv', index=True, index_label='device_id')\n",
    "\n",
    "print(pred_test_df.head(1))\n",
    "\n",
    "#generate prediction:\n",
    "# submission = pd.DataFrame(pred_test[:,0:12], index = gatest.index, columns=targetencoder.classes_)\n",
    "# submission.to_csv('/home/username/projects/talkingData/keras_cv10_plus_regression_80_reg.csv',index=True)\n",
    "\n",
    "submission = pd.DataFrame(pred_test[:,12:25], index = gatest.index, columns=targetencoder.classes_)\n",
    "submission.to_csv('/home/username/projects/talkingData/keras_cv10_with_bags5_wEvents_AllData.csv',index=True)\n",
    "\n",
    "\n",
    "#generate mixed prediction (no events=logistic; events=keras)\n",
    "\n",
    "pred_mix=pred[:,12:25]\n",
    "pred_mix[train_id_ne]=pred[train_id_ne,0:12]\n",
    "\n",
    "score_val=log_loss(y, pred_mix[:,0:12])\n",
    "print(\"Mixed ne:Logisttic: logloss for {} folds is {}\". format(n_folds, score_val))\n",
    "\n",
    "pred_test_mix=pred_test[:,12:25]\n",
    "pred_test_mix[test_id_ne,0:12]=pred_test[test_id_ne,0:12]\n",
    "\n",
    "submission = pd.DataFrame(pred_test_mix[:,0:25], index = gatest.index, columns=targetencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
